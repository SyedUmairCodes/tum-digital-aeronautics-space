# Introduction
Welcome to Module 2, introduction of our online lecture, Digitization of Earth Observation. We will look into, where do we come from? What is the definition of remote sensing? Why are we doing so? Earth observation is taking place today, in a very strongly changing and fast changing environment. We have all the global trends of digitization, big data, Cloud computing, very high networks, mobile access to data. On the other hand, we have our changing world with all its challenges, climate change. We have more and more people on this planet, and the challenges which come with that, which asks for more and more data about our environment and about our planet and this is also why Earth observation has become the biggest area of space activities worldwide in the last years. So if we look into what happens on our planet, I show you here is the expected development of the population on our planet. We are at a moment when we take this lecture to out of 7.7 billion people on this planet but we expect until the mid of this century about 10 billion. The Food and Agriculture Organization of the UN in Rome has estimated that, then we will need about 85 percent more food on our planet. That is in 2050, so we have only 30 years for that. We need to produce this food without destroying our environment completely. So we are in a nexus of climate change, water and food, because food production means all of water and water supply and we have to manage our planet in a way and these calls in a way that it is rarely to the benefit. In this context, the UN has also developed, the Sustainable Development Goals, which set 17 goals and Earth observation can support many of these goals, especially in managing developing countries, supporting agriculture, supporting water management, and finding, for example, locations for solar and wind energy, city management monitoring climate change. So this gives you an idea on [inaudible] home of earth observation why we do all that? Why we put all this effort into launching satellites? But also into the technologies to distribute and analyze the data. But let's go one step back. Before we do that, what is remote sensing? What is Earth observations? Let's look into the definition of what we want to do. So remote sensing is the acquisition of information about an object without physical contact with the object. So we are not there. It's from remote and we don't need this physical contact. Remote sensing is used in many fields, but in the space age, it generally refers to the use of satellite or aircraft based sensors. We will see this. When the Earth is the object of remote sensing, because of course we can make remote sensing also with planet and other objects. So if it is the earth, we speak about Earth's remote-sensing or Earth observation. This is also where the title of this lecture comes from. Here are some applications of Earth observation, which I think for the introduction I would like to mention also to give some motivation. We speak about monitoring of oil spills. We can measure land subsidence from space with radar technologies. We can make visible tectonic movements of the Earth. We can observe ships in our oceans, we observe volcanoes, floods, deforestation, vegetation and so on and all these from a unique observation point, from satellites. Satellites don't know, about us and we get all the data. This is the same system and this is the same methods and then we can aggregate them to a global view about our planet. So this was what I wanted to introduce you before we start. The next module will be about the data sources.

# Data Sources
Welcome to Module 3 Data Sources of our online lecture digitization of Earth-observation.
Play video starting at ::22 and follow transcript0:22
We will look into, as the title says, where do the data come from. And so the question is which sources of homemade sensing data do you know. And as the first source of observation data, we will look into in more details satellites. And we'll look into a into a system, into a platform, which is called High Altitude Pseudo Satellites, its new possibilities of platforms carrying cameras and maybe also the radars one day, between satellites and planes and send some more classically planes, which are still used for many observations data, and as of course category unmanned aerial vehicles or drones, which are more and more used for many applications and observation. And all these data go together and are processed together nowadays. And you should know different specificities. So let's look in the beginning. So a bit humanistic approach in the beginning of remote sensing. I show you here this side with pigeon cameras in Germany in 1903. It was the first tries to connect, in this case, still analog cameras to pigeons, and the birds were flying and taking the first photography. But a bit more CIS in the 60s. We had the first digital images from space, 1960 from the US Vessel Satellite TIROS. It was a spinning satellite. As you see also from the form of the satellite, it was spin stabilized. And also the imager was spinning. And therefore with each turn of the satellite, one line of the photo, the image of our planet was taken. And this is a technology, which is still used in some generations of weather satellites, which observe our planet from geostationary orbit. Then came the first high-resolution imager from space system, which we have longest is Landsat, which I would like to mention. It started in '72. And since then, we have uninterrupted data, which are all compatible. And this is important. This is why I mention it because in this observation, we not only have to understand what is happening now, we have also to compare that, which was happened many, many years ago. And we when we speak about climate, we speak about 30 years at least and longer. And here, we have now already data for 50 years from the Landsat CIS. So Landsat CIS was then followed by the first European system spot from France. And this came in the '86 time frame. And also nations joined shortly after India started its first optical amateur. And all these theories have been continued until today. So we have a couple of systems, governmental systems mainly in this time in the optical sector from where we get data for the last 30, 40 and 50 years.
Play video starting at :3:53 and follow transcript3:53
A bit later, in the 90s, came the radar technology, which has become very important for earth observation because radar gives us asset type of information about our planet. Radar can look through the clouds and also by night. So we are not dependent on sunlight and the illumination and clear sky. And therefore radar was developed. And the first really operational radar systems came from the European Space Agency with ERS1, ERS2, later Envisat in '92. It's a bit of first launch. And since '92, we have now also 30 years more or less of radar data uninterrupted. And also, Canada joined with its own system, which is very similar to the European approach, and Japan with an other wavelengths looking with L-band radar with its GI system and follow-on systems, delivering us also data in this wavelengths of radar. So historically, we have seen this. And I would like to show you two reasoned systems. I mentioned already when we spoke about the objectives of the lectures that
Play video starting at :5:12 and follow transcript5:12
I will always give two examples. One is a public system, and I have chosen Copernicus for that. It's a European system for global monitoring for environment and security, which has been built up and which is accessible for everybody. And a commercial example for very high resolution imagery. This is what the commercial world is covering with the airbus intelligence satellite constellation. So let's look into that. First Copernicus, as I said, the European Union, some time ago, has decided to build a system, which turned out to become a real game changer for the whole earth observation world. So a fleet of satellites is in Orbit 7 at the moment, radar satellites, optical sensors and also altimeters and hyperspectral images, as well as atmospheric chemistry instrument.
Play video starting at :6:20 and follow transcript6:20
As I said, it's already in orbit. The satellites are called Sentinels from the Italian Sentinel, the guardian of our environment. It was the idea when we when we define set.
Play video starting at :6:33 and follow transcript6:33
As I mentioned in the introduction, I was myself responsible for implementing system. I have also helped many years ago to define it in the declaration. And so nowadays, it's fully operational, delivers every five days a complete coverage of the planet surface with
Play video starting at :6:56 and follow transcript6:56
images or every six days with radar images. And this is for the equator. If you go in higher latitudes, then it's partly daily, which is a real novum in earth observation. Copernicus was built for some hundred governmental services. But meanwhile is it has this open data policy. It is used by many, from scientists to startup companies, value-adding companies, private persons. And it gives a lot of input to policymakers in many decisions we have to make about our planet and about the management of our planet Earth. There are different services built on Copernicus, an atmosphere monitoring service, a marine environment service, land monitoring service, climate change service. An emergency management service and also service on civil security. So this is all built on top of this satellite infrastructure, which is existing. And to our surprise positive surprise I have to say, the number of users has superseded all expectations, so we are in the order of 300,000 Copernicus users meanwhile. When we when we take this module and this was unexpected. As I said the design was originally for some hundred governmental Services, which are also existing of course. But there was a big pickup of data analysts, this data is really repetitive almost daily available and free for everybody. So besides this public system as on the data acquisition side. We have also commercial systems and we have divided a little bit this world by resolution. So the public system goes up to down to about 10 meter ground resolution, sometimes a bit better. And the commercial world is concentrating itself on the very high resolutions that means from one to two meters down to 30 centimeters. This is what is at the moment commercially available. And what you see here on this slide is a few on the satellite constellation the company Airbus is offering with some radar and optical satellites. And here you see a bit the capabilities very high resolution comes usually with a smaller swath width. That means you can a bit less of the Earth's surface when you fly over it. And so when you go on the very very high resolution end you have Pleiades Neo, two satellites to be launched in 2020 at the time of taking this module. They are not in orbit yet. But I hope a bit later in this year. And then with two more launches two years later,
Play video starting at :10:13 and follow transcript10:13
then 30 centimeter data will be available, optically. What is already in orbit is the Pleiades 1A and 1B with 50 centimeter resolution, and 20-kilometer swath width. And similar satellites, which are also in the constellation up to spot which are spot six and seven. I mentioned spot already in the historical introduction. We have more than 30 years uninterrupted spot data only sets the resolution has become better and better and spot 6 and 7 deliver 1.5 meter resolution at the moment. And then we have the radar satellites, at the moment this is tanDEM-X TerraSAR-X and PAZ, three satellites which are in the fleet of the commercial offerings of Airbus. And they deliver radar data very high resolution also down to 25 centimeters and with swath width they can as you see in this little video. Which is playing while I speak said there are different modes to to take data and depending on the modes the ground resolutions and swath width is changing. So this is a short look into what is commercially available. I would also like to mention that the data from satellites nowadays are not only downlink directly, which is a more classical approach, but a new system of data relay is used [COUGH] which is based on laser. Copernicus again is the pilot customer let's say, so four satellites out of the seven there with images who have a very high data throughput the radar and the optical images. So Sentinel 1 and 2 A and B respectively carry laser terminals and they relay the data while the fly on a low earth orbit up to a geostationary satellite and the data are sent down linked to Earth. This has solved the problem that we have always to overfly a ground station. Otherwise when we have not a data relay system, so the data are much earlier and faster available than without a relay system. The same terminal is available on TerraSAR-X and will also be available on the next generation of the Pleiades Neo which will be launched in 2022. Let's now look into another platform from satellites, which fly usually in the low earth orbit 800 kilometers about sun-synchronous orbit. We go very much further down in an altitude of about 20 kilometers and here a new technologies are coming up. They are not yet fully operational but really in the making and many hands we call them high altitude pseudo satellites. Pseudo satellite because they fly higher than planes. And mainly three principles have been developed to keep a platform as long as possible in an altitude of about 20 kilometers. And this is the balloon technology is the most simple one which you can use, stratospheric balloons are in use since many many decades for metrology. But this time the balloons carry cameras to make photos. Then type of planes but planes flying much much higher and planes who have to be in the air much much longer. So we don't speak about hours or a day in in flight, we speak about months in flight. And the target is to have 365 days per year the plane in a kind of orbit in the air in the stratosphere. So this is a technology likes the Sofia project of Airbus. We will look into that which Technologies are necessary. And then there is the Zeppelin technology as potential third technology. Which is also used for example by the strata bus which is an as a concept you can use.
Play video starting at :14:48 and follow transcript14:48
What are potential applications? So the idea is that you have a platform which can be fixed like a geostationary satellite but much closer to a city for example,
Play video starting at :14:59 and follow transcript14:59
maybe in some years from now at least over the mega cities of the world. We have always flying platform from which we can monitor the traffic or the pollution in a city or also we position such a platform over emergency areas after earthquakes or volcanoes. This has already been done I will show you this. You can also put on as payload on those platforms communication
Play video starting at :15:32 and follow transcript15:32
transponders to have after an earthquake for example, very fast a new mobile Communication System running. You can use it for maritime surveillance for news broadcasting at the end if you have Enough capacity on board also for a smartphone apps which are directly connected. So all these possibilities are Coming up, you see when we speak about the plane technology, we speak about enormous planes with electric motors, driven 20 meter wing length. For example, Zephyr S with flying in the 20 kilometer altitude, and being covered by solar panels because the electric motors have to be fed and you cannot take fuel with you. So the solar panels and the batteries sets must be
Play video starting at :16:32 and follow transcript16:32
powerful enough to keep this in this high altitude. Again, the target is to stay there for maybe a year. But at the moment, the world record, and it was set up in 2018 by Zephyr S of Airbus is Quasi 26 days, to be exact 25 days 23 hours and 57 minutes. It was the longest ever flight of an unmanned plane. And so the technology has been really demonstrated. To give you an idea, the whole plane is 25 meter wingspan, weighs only 75 kilogram. And you can imagine that this is also a challenge then. For example to build very, very light weight sensors.
Play video starting at :17:23 and follow transcript17:23
So what is the best flight altitude? We have roughly said okay, we want to fly in the stratosphere. So above the troposphere where we usually fly and the weather is taking place. So what our criteria is for the best flight altitude says, okay, there should not be so many clouds anymore in this altitude. It should be above the air traffic because it would be too dangerous to have quasi stationary place in Z or Zeppelin or balloon in an altitude, whereas a normal air traffic with thousands of movements goes. So it should be low wind speed because of course, this ultralight constructions platforms. They are all dependent on the wind for the balloon technology. It's even a disadvantage because they drift with the wind. So they will not stay long over the same point and you have to launch too many. So it's all a question of costs and here is, the answer to set can be given if you look a bit in the two diagrams on the left which shows you the wind speed and we see altitude. And it shows you that you have to go above the jet streams. You have a certain you have to go above the tropopause, that is the area where normally most of the weather takes place. There are still, where also most of the clouds are. There are some clouds, especially cirrus clouds in the stratosphere but not too many.
Play video starting at :18:59 and follow transcript18:59
So we have to go into the stratosphere. The air traffic stops at about 11 kilometers altitude and the jet streams as you see from the diagrams at the left side of this slide say also mainly in attitudes slower than 17 kilometers. So if we take all these together the ideal bend to be with an high altitude pseudo satellite is 17 to 22 kilometers. And this is exactly what is targeted by all these technologies. So let's look into some applications. So here for the balloon technology, which is already operational. There is Loon, is a company, sister company if you want of Google and Alphabet say have already demonstrated with stratospheric balloons said you can have payloads too. We built after for examples, if lot in Peru and the hurricane in Puerto Rico to rebuild an LTE communication, mobile communication system coming then from the balloons because the whole team masks on ground were of course damaged by the natural disaster. So say we are in 20 kilometer height for more than hundred days. But of course, they are drifting so you have to relaunch stratospheric balloons and you also lose the stratospheric balloons and at a certain point. But this is already operational Zephyr S will come very soon. The first test flights are now taking place and also the chaplain technology will come.
Play video starting at :20:45 and follow transcript20:45
We said I would like to move to the more classic a platform planes say are used for many years now. And are already standard for applications like photogrammetry, for mapping. So most of the topographic maps in the past were made by
Play video starting at :21:6 and follow transcript21:06
airborne sensors by stereo cameras for the committee cameras. They are partly replaced nowadays with very high resolution stereo photos coming also from satellites. But they are still a very important platform. Also, hyper and multispectral scanners, for example for agriculture, flying on planes. Raiders, altimeters, laser scanner. So very different types of earth observation sensors based on airplanes. And also atmospheric sensors for air quality is also validation of satellite instruments, because whenever we make a satellite mission, we have also a series of under flights to validate instruments in orbit. So what are the advantages of planes is still the very high resolution because you fly low, some thousand meters. Usually you have some hundreds. Even you can carry big instrument compared for example to Hubs and you have a reasonably stable platform. The disadvantages on the other hand is that they are expensive, you need a pilot. You have small areas which you can cover compared to satellites. You need permissions and you are very weather dependent. So depending on these different pros and cons, you see also, what is the criteria to choose these platform for your observation sensor. And the last I would like to mention in this model so-called UAVs or unmanned aerial vehicles.
Play video starting at :22:45 and follow transcript22:45
It's not only a toy, of course our kids really like to play with UAVs but there is also very serious market for them. And professional UAVs with cameras, with multispectral cameras also, which are used today already operationally for agriculture. But also for example, to assess storm damages after the storms. Or for media. And again, here on the advantage side is said to have a very high resolution. They can fly some 10 meters high. They are easily deployed and very easy to transport as the costs are low compared to all the other systems.
Play video starting at :23:29 and follow transcript23:29
Disadvantages again, you still need pilots. You have a very small area which you cover compared to satellites. You also need permissions and you are also in a certain way weather dependent. So again, all these platforms are complimentary. From all these platforms we get similar data with various different resolution on ground. But they are very complementary and we have to regard all of them nowadays when we speak about earth observation. So with this we are at the end of this module on data sources. And I would invite you to the next data module which will give you insight into data archives access and processing systems for earth observation data. Thank you for attention.

# Data Archives & Access and Processing Services
Welcome to Module four of our online lecture, digitization of remote sensing. It has the title; Data Archives, Access, and Processing Services. It is covering the ground infrastructure which is necessary for Earth Observation Systems, and which are nowadays the key to the success of the Earth Observation Systems due to the high amount of data. Let's look first into the data access. Data access means how can they user access the data, also this huge amount of data. So what we need is a user interface, at least a catalog system, an ordering system, and a download functionality. If you look in the past, before the World Wide Web and web browsers, we had dedicated clients running on each PC computer of the users. Later, we used World Wide Web as user interface with FTP downloads. Today, the download comes from what is called hubs, that is physically data archives which are connected to a broadband network,
Play video starting at :1:33 and follow transcript1:33
or even more modern in the next steps in Cloud-based archives.
Play video starting at :1:43 and follow transcript1:43
The disadvantage of hubs is, of course, that you need to download very big amounts of data, we speak about many petabytes.
Play video starting at :1:56 and follow transcript1:56
This big amount of data can block even the very broadband networks and take a long time. So there is the newest trend to have more and more data Cloud-based to replace physical archives which are slower by Cloud-based archives, and then to download from there. Or as we will see in the next chapters of this module, to process the data even in the Cloud, because this is a possibility when you have the data in a Cloud, and then avoid to block our networks with this high amount of data. I will give, again, two different examples. Although in this module, the one is what is called DIAS or Data Interface and Access Service of the Copernicus system, and it's for public offering. The other is, again, a commercial system. It's OneAtlas Cloud platform of Airbus. To give you an idea of the amount of data we are speaking about when we record this online lecture, the archives of Copernicus are about 16-17 petabyte, and every day about 10 terabytes are coming on top. That means about 3.5 petabyte per year comes on top. So it's really a big amount of data, and this is only the satellite part. There is other parts we're speaking about. Here you see a diagram of the Copernicus archives was the different excess capabilities which exist nowadays. So you have the classical hub, which is still available and in parallel to the DIAS Systems.
Play video starting at :3:49 and follow transcript3:49
So it's another bubble which indicates the additional functionality of processing in the Cloud, which we will look at a bit later. On the other hand, the OneAtlas platform Cloud archive from which you can download the very high resolution data on the commercial side, if you are an inscribed user, and where you have also interfaces to the Copernicus system. So also the Copernicus data are downloadable. It's via one of the DIAS's. I will speak about that. It's called Sobloo. There's another bubble in this diagram, which is for data processing in the Cloud. It's another offering. It's called Up42, I will show you this example when we speak about the processing services and more. So SSN all ready to avoid downloading all this big datas, and in Copernicus for example, this 16 petabyte of data in the archive, statistically, every datum is downloaded 10 times, so this immediately multiplies into something like 160 or more petabyte over the time which users would download. To avoid that, we have nowadays the possibility to process in the Cloud and download only the results. So I have here as example the Analytics Toolbox in OneAtlas. You see here on the right-side different functionalities and analysis tools. When you offer on a Cloud platform a toolbox software which you download and then you process it on your own computer, so this is called Software as a Service, SaaS. These terms come from the National Institute of Technology as definitions, and these definitions go far beyond earth observation. It's for the whole Cloud IT activities, the same terms are used as well. So, new Cloud platforms offer software collection. Another example is the ESA toolboxes which are available. You can go on esa.int, and in the earth observation part and you will find all the toolboxes for all missions of ESA, for all type of earth observation data. So Software as a Service is already reality. Another acronym you should know is PaaS or Platform as a Service, this means that a computer platform for the development of applications is offered as a Cloud service, and here a good example is Up42, which is an Airbus startup based in Berlin, which offers for geospatial data, a platform where you can build your own business as a services. But let's look first into the access to Copernicus data. I already mentioned that this data open and free, which has brought about too in also the number of people working with this type of data. As I mentioned, about 300,000 people are downloading Copernicus data at the moment. It's a moment where it has a hybrid system, that means you have access either via the so-called hubs. So the Copernicus Open Access Hub would be, for example, the hub you as a user would choose. So again, if you go to www.copernicus.eu, you can find the data hubs and the links, and then you can download data directly. Three hours after acquisition, Copernicus data usually available on the hubs in a processed form, and then you work further with them. There is also other elements of the ground segments, which I described here. So as the Copernicus Space Component Data Access, C-S-C-D-A. That is to offer also access to certain collaborative ground segments. Collaborative ground segments is ground segments which are built by third parties connected to Copernicus and it's also very useful and usually you'll find many additional data there. There is also, EUMETSAT, so European Meteorological Organization which has connected to the Copernicus systems. For those who come from the meteorological end, they can access the data via EUMETCast, which is the distribution network for meteorological data and this is also open concerning the Copernicus data. Then there is thematic hubs, which you see also in the graphic here. They don't offer the original satellite data, but they offer data which were processed to certain products. There is thematic hubs for land, atmosphere, emergency, marine, climate, security data and services. So here, depending on the user community, you'll find derived data, value added data coming from Copernicus also from the Copernicus services, which are operated by the European Commission or by organizations which do set for us the European Commission. Then parallel to the hubs, we have the DIAS. Data interface and information extra service to access the data which are on the Cloud platforms. EU has decided that there should be five in parallel. It's PPP, so public-private partnerships. It means that the partner who is operating and has created the system has chosen more or less the composition of what is offered. It's usually more than Copernicus. For example, sobloo. This is the one which Airbus says is running for Copernicus. Contains also access to OneAtlas, to the commercial data of the Airbus satellite fleet. So the users who are interested in the combination of two, find here both the same tools for the other DIAS Systems, which come from different ends and you'll find the link here on this slide for all of these, you should try out what is the best for you. If we look on the commercial side again, in the commercial Earth observation Clouds, OneAtlas, I already mentioned, which is the access point to very high resolution industry imagery and certain evaluation tools and analytic tools for processing services. So it includes a GeoStore. It includes also what we call OneAtlas Sandbox, which is an area where you can test and develop applications, new analytics. It is also possible to connect to its Application Programming Interface, API, which is what is used for example, if you want to have sent results of what do you do, what you analyze in the Cloud, on your mobile phone for example or computers and you make this accessible via the Application Programming Interface. Then, there is the open platform, Up42. It's a marketplace for Earth data and analytics, which includes again OneAtlas data and Application Programming Interfaces. So if you want to build a business, for example, based on Copernicus data plus OneAtlas data plus, many, many asset geospatial data, you can choose Platform As A Service provider, example here, Up42, but there are others on the market. Here you see a little video of what is possible in terms of analytics. In OneAtlas, for example, to give you an idea what you can do in the Cloud already without programming. Of course, you can also program your own application. So you can, for example, take satellite image. You count cars over the time, you count the trucks over the time, you count aircraft over the time. You can make certain land use classifications. You can distinguish water, buildings, streets, forests, vegetation. So since are all analytics which are already built in into the Cloud processing, and you can monitor in general changes of infrastructure or land use evolution. Very important information concerning the development of our cities or the course of population on our planet will be in cities in the future. Therefore, this is really an important focus also of the analytics, but also very interesting for the economies. So this application example. I would like to close this module, and I thank you for your attention. I hope you have learned a little bit about the importance of the ground segment and the processing part in the times of big data and cloud computing. I leave you now to the last module on artificial intelligence. Thank you very much.

# Analysis - AI4EO
Welcome to the online lecture, digitalization of remote sensing. We are at the Module 5, Artificial Intelligence and Data Science in Earth Observation. So our mission in Earth Observation, is that we research and develop solutions for major challenges in the society. For example, Earth system research, global change research, meteorology is a typical research area, and our sustainable development goals of the UN, and also down to very practical things like security, mobidity, resource management, and city planning. We are now entering the big data era of earth observation. This you have heard already from the previous module, and the game changer, as you probably already know, is the Sentinels. Through the ESA particles program we have now the Sentinel need, which provide free and open data to everybody who are listening to this lecture. More importantly, it also have a long term perspective. Basically now we have guaranteed data access at year 2030 and in ESA, people already discussing plans for 2040, and another aspect I would like to mention is that complimentary to this agency based missions, we now have entered blooming era of new space. For example, Planet Lab, is a new space company. They have more than 140 small satellites where their capacity is they're able to map the earth on a daily basis, and because of availability of this big earth observation data, nowadays we also have Internet giants and the startups who are entering EO, and the challenge we have, however, is that the classic methods we have for earth observation could not anymore be sufficient, were really lead Artificial Intelligence for earth observation, and this is not as simple as that we motivate some data scientists to join earth observation, instead because we have very high quality requirement for a wide applications. We really need EO specific AI research, and innovative AI for EO methods. Actually in this era of big earth observation data as competence, what do we need to have is, for example, data fusion. Nowadays for every single application we will not anymore strict ourselves to any individual data source. We have radar, we have optical sensors, we can also have atmospheric information, and so on. So therefore, it's very important that we try to get the complimentary information from different sensors for different applications. The second is data mining. So we are now confronted with petabytes of available data. So how could we dig out, actually is a relevant information for us? Also if we have time series data how could we without any supervision, to find out if there any interesting phenomena happens such that you don't have to scream all the data you have, and at the third point is machine learning and deep learning. So this will be the main topic of this module. I will talk about this later. Last but not least, also Big Data Management and HPC. So if you imagine that you are doing any kind of global research, you will be asked to be processing, and managing petabytes of data, and for sure then you'll definitely need the competence that you will be able to have smart EO database, and you will be able to use the supercomputers for this task. As I was mentioning today, we mainly talk about machine learning and deep learning, and actually 20-30 years ago, there was a wave of new networks because actually scientists has demonstrated such kind of neural networks is very powerful. You're modeling non-linear process, for example, if you would have two hidden layers. Theoretically, it is possible for you to model any non-linear process. But after a while, the window cut came, The main reason is threefold. Firstly, we did not have actually enough training data at a time, and also we don't have smart training algorithms which allow us to train models with a lot of parameters, and then the third one is we did not have the computing capacity for doing that. Actually exactly because of the breakthroughs in these three directions since about 2006, we now have a high season or for deep neural networks, which is neural networks which is much more hidden layers compared to the shallow networks, and we have a lot of breakthroughs in many different fields. I don't need to mention all of them because you are reading this already from the news. Let's look at what is happening actually in remote sensing. So this is some literature research we did actually in the end of 2017. If you look at the papers which is related to deep learning remote sensing, you will see it shows really much more than Linear increasing much, much more papers are published on this topic. So I would say this is a hot topic, maybe even already a hype, and we have been actually observing the whole community from the beginning, and what we observed is there actually clearly three-phases of deep learning and earth observation. Phase 1 we call it quick wins and quick papers. This is basically, people get excited about the power of deep learning, and the [inaudible] networks and to do the things, what they can't do yesterday, but now with deep learning with two percent improvement of the accuracy, and about three years ago, we start to understand, "Okay, earth observation is different from Internet imaging labeling." Therefore, we start to design new architecture that are fitting perfect to earth observation data. We also start to train models really from scratch, and about one year ago then people again figure out, "Okay, even though I tailored architecture with earth observation data, still I'm considering this as black box." We do have actually decade of EO expertise. We should try to make use of them. So this phase is called opening the black box or turn the black box gray. Basically, we want to implant physics space, and the domain expertise into the learning process. So here is just an example of our Phase 1 success. We're participating the Data Fusion Contest in 2016, and at that time, we are using Deep Neural Networks to analyze those space videos. Eventually, we were able to also get traffic density from a video which is taken from the International Space Station. At that time, we actually won the Data Fusion Contest, and I always say this is probably a big part, is because we are the first ones who are actually making use the power of deep learning. Under such circumstances, of course, we're not happen anymore today. What I would recommend young scientist is that, if you are using deep learning for Earth observation, you need to think about what actually makes it a special for Earth Observation use case. Here I listed a few of them. The first one is that, classification and a detection, which is the problems we usually have in the computer vision community, is actually just really a small fractions of earth observation problems. Instead, we're rather focus on the retrieval of physical and biochemical variables and we also have very high accuracy requirement, we did traceability, reproducibility of the results and more importantly, we should be able to quantify the uncertainties in our results. Any measurements without an error bar has no meaning in us observation. Domain expertise. This I mentioned already, we should make use of it, and compare to regional community where we have for internet images, we actually have well-controlled Data Acquisition, which is the radiometric geometric spectrumetically, wherever well-calibrated. For many cases, we even know the signal-to-noise ratio of our data. Our data is also different from your handy images, which has red, green, blue, three channel. Our data is the so-called five-dimensional data and sometimes it's complex valued if you'd think about radar and also multi-modal if you try to combine different sensors. What is a five-dimension? X, Y, Z is, of course, this 3D world coordinate. Then we have the tie, as I was mentioning, something has provide more or less a weekly measurements of geolocations and a lambda stands for the wavelength. So we have a different sensors ranging from visible light, tier, microwave range. We can even also make use of for citizen science, social media data, etc. Anyway, the data is much more complex than the internet images we have. Also, one downside of earth observation is that, we don't have ImageNet, which is more than 40 minion labeled images. In many of the cases, we're actually lack of a sufficient training data. So these things you always need to consider if you try to make use of deep learning for us associate earth's observation problems. Today, I give you a menu. This is basically the deep learning research we do. For example, we are detecting tracking of ships and the vehicles, we do the segmentation, classification of buildings, slums, and if we have the global data, of course we can do the land use land cover classification on global scale. We can even go to individual building level to get the sediment types, or if it's related to climate research, we could also classify it as a local climate zones. This is more detection, segmentation, classification tasks. We can also analyze time-series data to detect changes in the certain geolocation, and a very important aspect is data fusion. How could we match sun optical data, which is actually not as trivial as you may think, and also how could you fuse different type of data like optical SAR, polarly metric, lidar data in 2D and 3D if it's in particularly in very high resolution, and how could we synthesize optical data from SAR data and the vice versa, this is of course very relevant if you want to combine sun optic data, for instance, for change detection, and how could we remove clouds from optical data and we can even do more fancy stuff. For instance, we can try to estimate the beauty hides from one single image and then we could try to detect all the building footprints from a single image. We can even fuse earth observation data with social media data and the list actually goes on and on and it today and here is basically a small selection of the tailored deep-learning architecture we have developed. This is what I usually call our d plus two, but today we will not go too many details of them. Instead, I want to showcase use on successful examples. The first example is related to hyperspectral image analysis. You know, the hyperspectral sensors that have hundreds or for continuous bands, maybe rubber bands. How such a sensor is that you can actually discriminate different material in the SIM. One actually practical problem we have is the so-called inter-class variability. So basically what do you see on the screen is a patch of tree. If we plot the spectral signature, you see, of course we have detailed spectral information. But even for the same material, this show vulnerabilities. This is a video I would like to share with you where you can see how other ones, machine learning algorithms can extract discriminative features from this type of data. So you can see after the integration actually spectrums of trees, soyas, packing nods they're separately much better than a batter. This, of course, helps us to make use of hypespectral data. What do you see? The most stretching one is the soya. So this is also very reasonable because it has a much stronger spectrum variability compared to other classes. The second example is a time series data analysis. Here, I show case you one of actually state of the art architecture we could use. On the left-hand side of the screen, you see the images from different times. We first use a convolution sub-network to extract the high level spatial and a spectral feature from the data, and after that, we will stack as kind of a recurrent sub-network to model the sequential behavior, and then afterwards by sigmoid or softmax layer, we are able to get the binary change mask, change and no change, and even math class changes. What has being changed to what? Here is the results you can see. Left hand side is a multi-factor data with three-year interval in the middle red color means where change actually happens, and on the right, different colors stands for different type of changes like city expansion. So what a change. This is the general framework. Of course you can do it also is a time series data. Basically here you see with 30 years of data, you see how a city like Beijing has been expanding and the more yellowish color means it's a newer urbanized area. The last example is about Munich Airport usually make joke about this results because you can see from the past 30 years, how airport has be gradually build, this is more or less the evidence, the most expansive evidence of the time-consuming construction progress in Germany, but I always say that maybe it was data of the next 30 years we can prove the German engineering quality as well. For other example is about High Resolution Remote Sensing Image Analysis. So if you have optical data, in this case is from planet, is a new space company and if you generate also grant was for instance, using open street map, and we fancy architecture here is a graph convolutional recurrent neural networks, you can train it and then finally, you will be able to extract the building footprints from the optical image, which is about three meter resolution. Also if you use multitask of kind of network, you can also get building instance. Here different color stands for pixels, belongs to individual buildings, and you know, planet data is globally available. Basically, with such architecture, you will be able to map or actually the building footprint and build instance on a global scale as well. If you have videos, you can even do more fancy stuff. What you see on the screen is that, we are detected not tracking the vehicles and here we're using a rotatable angle and the strength of such a technique is that you will not loose the cars if they're turning. We can even apply deep learning for citizen science. If you have social media Street View images and you can actually use them to determine the building functions, how could we do that? So from remote sensing data I showed before, you can get the building instance. If we would have a library contains more than 10,000 for ground level images of our building facades, and then we give a label to them, of course then, you can store it into a deeper neural networks. Finally, you will be able to get the building functions for different cities. Here is example of Chicago, Vancouver, and Munich, and with about validation with seven cities, with the Street View data alone we are able to reach accuracy up to 80 percent. But of course, for actually larger scale study, we need to do more research on that. The last example I prepared is Global Application with Sentinels, and also actually another very concrete application. If you're working with the Sentinel-2 data, the biggest problem you usually have is the clouds. Basically, here is the statistics showing that 67 percent of the Earth's surface actually covered by cloud. There are a lot of the previous study which assume the cloud is only visible in the visible bands. But here, on the top you see actually they're everywhere. So what we are doing here is we're using a conditional GANs. So input is the cloud in Sentinel-2 image. We are using the Sentinel-1, which is a radar image independent from weather, to make use of the geometric information as the condition to produce cloud-free Sentinel-2 images. If you look at the preliminary results on the left, you'll see the Sentinel-1 images, the condition, and you'll see the Sentinel-2 inputs with clouds, and in the middle is the ground truth. On the right is what we can get from a deep-learning architecture. So you see it's actually quite promising. Now we come to the concrete application that I was talking about, is the Global Local Climate Zones Classification. Why I'm mentioning this example because I want to demonstrate you the actual work behind application, if you want to really do it on a global scale. So local climate zones is actually a schema, actually proposed by urban climate ontologists, and it has 17 class describing basically the compactness, height, percentage of green of the urban area. It's more like a morphological description. We want to do this on a global scale, and also because actually we are interested in semantic functions of the urban structure. Why this can help, I show you a example, the sevens class, lightweight low-rise. It will actually fit very well to our expectation of the slum areas, which is actually what we care a lot. Now, coming back to the global LCZ classification, if you are working with machine learning and you want to do a task, the first problem is always how could we generate a training dataset? So in this case, you see that we actually hand-labeled 42 cities selected across the globe covering all the culture zones. Also, we give labeling confidence, which means for each label we are letting 10 experts to give a vote, such that there is also a way to quantify the uncertainty. In the end, it ends up about 400,000 pairs of Sentinel-1 and the 2 image patches, with the labels and the label confidence. We actually made these free accessible. You can also download it in a trial with your trial examples. This is a benchmark we choose, it's very suitable for image classification, data fusion, quantification of uncertainties, computing in AI, and even automatic deep topology learning. Here, I'm showing you the first global urban local climate zones classification we derived. So basically, what we see as color, this is not a simple base map, it's actually the local climate zones classification with the schema I showed you before. Of course, we are more interested in this reddish area. These are the urban areas. I can show you tonnes of classification map, but I decided to show you a very detailed example, which is Munich. So the background you see this buildings are the 3D building models, and we overlaid with the LCZ classification. Basically, you can clearly see the dark red areas as a compact areas. If you look at the orange areas, these are areas with a lot of green, this is most stands for the rich area. If you look at gray areas, this is more industry areas, and you can also even distinguish dense trees, scattered trees. By the way, we are doing this actually on a global scale, both 3D and as LCD. So basically, we wish to have this data also openly accessible for everyone, including developing areas. Well, these are the examples I want to show. As a last slide, I would share with you the current research agenda we have. Firstly, we are always encouraging young scientists try to actually make AI4EO to tackle grand challenges. Not do the things which you can do yesterday, but always two percentage improvement. We can do things contributing to monitoring urbanization, try to derive indices for the sustainable development goals or we can even deliver geo-information for climate change research. If we look at the open issues about AI4EO, one very important topic is how to re-implement physics-based and the domain expertise into deep learning. Another aspect that I want to mention is reasoning. So far, basically all AI4EO research are still stay with recognition, so you know where, what. But how could we then derive logic other than that? The special relationship for different objects and which kind of phenomena it is, so this would be the next step we need to go. Also, if you want to use deep learning for global research, transferability is, of course, a very important topic. You cannot use your model trend in Europe simply in Africa, that would never work. Name a few more examples, it's very important to quantify the uncertainty, this means both, actually the model uncertainty and also the data uncertainty, we need to actually make thoughts about this before you start the whole pipeline. Explainability of the deep neural networks, computing in AI, and the last point is, actually I want to make awareness, is the ethical aspect. We are now having actually global data for many of the applications which we could not do yesterday. We have a very good wish to help to improve the society. But on the other hand, we really need ask for guidance to make sure such information will be not misused. For instance, by some authorities, simply use this information to throw out people from slums. These things we should actually make clear thoughts and try to follow the guidance. This brings me to the end of the module and I thank you for your attention.
