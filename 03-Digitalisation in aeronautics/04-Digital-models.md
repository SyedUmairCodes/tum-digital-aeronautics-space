# Parameterised reduced-order simulation models in Aerospace - Hard computing methods for well-structured problems and models

Welcome to our lecture on parameterized reduced-order digital models in Aerospace, which is composed of two parts. Namely part A, which we will discuss today, which I called Hard Computing Methods. We will also discuss later, part B, which is called Soft Computing Methods. And both parts are parts of our Munich Aerospace online course on Digitalization in Aeronautics and Space. Let's look to the title, which sounds a little strange. First of all, the question is, what do such reduced or surrogate digital models mean?
Play video starting at ::53 and follow transcript0:53
The second point is, why do we talk about their parameterization? And the third point is, why do we have two parts, part A and part B?
Play video starting at :1:4 and follow transcript1:04
And today, we will focus on part A, namely the fundamentals of the related methods. And equally important, a series of applications, and not only from a numerical and digitalization point of view, but also from a physical and technical background point of view.
Play video starting at :1:24 and follow transcript1:24
Before we jump into details, I would like to make some remarks on myself. After having finished my doctoral dissertation at TU Darmstadt, I was working for 20 years in aerospace industry. Followed by 20 years in TU Munchen at the aerospace department and with lectures on aerospace structures, materials, and related subjects. So the detailed points we are now looking more into is, what do these reduced digital models mean? Why do we want to parameterize, and why is our lecture composed of this part A and later on the part B?
Play video starting at :2:7 and follow transcript2:07
So let's discuss this with some typical example. And first of all, we have to observe that in the, especially in the development but also in the operation of aerospace systems, we have to observe parameters which might vary a lot. For example, in operational conditions, the operational parameters which relate to aircraft are the payload mass, composed of fuel and passengers. And also the center of gravity of fuel and the center of gravity of the aircraft, the Mach number, the flight velocity, the flight level or flight head, and so forth.
Play video starting at :2:48 and follow transcript2:48
And in order to achieve a good design, we have to treat in the development and design phase possible structural and generally system design parameters. Which are the plan form of the aircraft, which are certain shapes of the wing and of structural parts, stiffeners position, geometries, types of materials, and so forth.
Play video starting at :3:15 and follow transcript3:15
Generally in the design and development phase, we want to achieve some goals, but most of all we have to satisfy a lot of requirements. And the important point is that these development design steps
Play video starting at :3:32 and follow transcript3:32
very much depend on digital models where we carry out the optimization steps to achieve the goals and to satisfy the requirements. So let's continue with this example by looking on the dynamic gust load alleviation in aircraft wings. Which means that we first look on to the stationary lift distribution, neglecting effects of the fuselage, shown here on the dotted line. Then we have the gust load effect. If the aircraft is flying through a gust and excited by a gust or turbulence, the lift is significantly increased, especially so at the outer wing edges. Which means that the wing root bending moment is increased, which is not so nice, especially also for the aircraft structure and for the required mass and weight needed. And in order to control this and to alleviate this, we are applying the so-called wing load alleviation. There is a passive means, which means a proper aero-elastic tailoring of the wing such that it deforms under such gust loads in a proper way and does not increase so much the outer wing lift. And secondly, an alternative and often investigated also active means, namely the proper use and proper control of flaps in order to influence the aerodynamic forces acting on the wing.
Play video starting at :5:7 and follow transcript5:07
In the end, for such alleviation problems, we are talking about mechanical system or structural control interaction problems. Which means that we are dealing with a plant, the aircraft especially, which is disturbed from outside by disturbance W. And this disturbance and the output from this disturbance is measured by certain sensors, which the result of which is an input to our controller. Which activates the plant, the aircraft, in a proper way in order to optimize and maximize the performance.
Play video starting at :5:45 and follow transcript5:45
And to determine the parameters, especially those of the plant and of the transfer function of the controller, we have to deal with these transfer functions G and H. And the essential point is in the basic ideas that these transfer function are often a result of large system models, which specifically relates to the transfer functions G of the plant or aircraft. And this is the reason why we are looking to the condensation and reduction of such large systems in order to properly determine the interaction of G and H for the overall system

So let's briefly outline why we have lecture part A and the lecture part B in order determine such reduced models which are also parameterized. In the approach of part A, which is the subject of today, we are starting with large scale system model, which is drastically condensed and reduced, which we see in the center of this view curve, and the concept and idea is that the parameterization shall also work on this drastically reduced model. So in parametric studies and optimization studies and so forth, we then only deal with such reduced models, which from time to time, of course, might have to be updated which we will see later. Such a technique [inaudible] runs on so-called clearly structured problems. There's another technique which is subject of our approach in discussion in part B of our lecture later on. We again have two parameters, parameter 1 and parameter 2 on the x and y-axis. We have the response or output of the system, and we have some reference points which might be either taken from simulation, which might be taken from experimental results or databases, might be taken from knowledge bases, and so forth. These reference points help to establish and serve to establish so-called approximation functions. Instead of the overall large scale model, these approximation functions as functions of the parameters are used to investigate the system. In both type of approaches, our aim is a drastic reduction of the computational effort drawn to design and development process. But at the same time, we have to take care, set the surrogate response, which is given by this a circumference. The other [inaudible] value represent or sufficiently value represents, the high fidelity model, or in other words, the approximation error shall be small. So let's look into the basic and fundamental steps of such reduced and parameterized models which come from our part A type of problems and methods. We have seen that we are starting with a large-scale system model, which somehow is reduced to a small-scale model, and we shall represent the overall behavior. But if this small scale model is not yet parameterized, we have to parameterize and do the parameterization process again, apply to the original large scale model when we are modifying the parameter, and modifying of parameters is a process and step often done during the development phases of any system, including aerospace systems. So the reduced model has to be parameterized, so we are again starting from the large-scale model, which you'll see as symbolic representation on the lower and center side of the view curve, which is condensed, also which is represented as function of our parameters, which determines the behavior of the overall system parameterized. So we can investigate the parametric dependence and parameter study on this reduced model instead of using large-scale models and doing the condensation again and again, and again. So let's look into even more detail of this approach. Again, we are starting with the large-scale system model represented by a large-scale matrix representation, which shall be shown here on the left upper part of view curve. Often, different subsystems of the aerospace system contribute in a different way to this overall model, and this being said, different sub-matrices are contributing to this. One has to find, in order to reduce these sub-matrices and do superimposed sent to small-scale model on S to find a proper projector and from S to find from a mathematical point of view, projector operator. We, which protects this subsystem matrices into a very small space, which might be Krylov-space or any other small space. In vibration, it might be a modal space or other possibilities exist. But the important point is you see on the lower part. So typically such a projector is composed again, on large-scale matrices and this shows that the application of such a projector after modification of the system matrix due to parameter changes. This application and determination means a lot of computational effort and you chose [inaudible] is better to modify and to use this parameterization already on the small-scale model and the reduced system. So in order to really save computation effort in a sequence of parameter modifications, it's relevant and important set not only the models are reduced and the projector is determined, but also [inaudible] projector we is parameterized biases parameters, which influence the behavior of the system under investigation. This view curve summarizes still previous two or three if view curve, namely we have see large matrix, extra-large system model on the left-hand side, see contributions of the blue, yellow, red, cream parts of different parameters and subsystems. These matrices are projected into a significantly smaller space, where they project operator V and superposition of cis projections results into these other small-scale system model, which is shown on the lower part of these view curve. What's left over is, since we have to deal with parameter variation in the design and development process, the question is, how is this projector V to be parameterized? It's a parameterization of the projection operator V, and the contribution of different subsystems and sub-matrices is done in the following way. In principle, and in principle means there also again, different means to do this. One fundamental principle, and often used is, to use linear or non-linear tailor series expansion. Let's assume that this is matrix A, which we are parameterizing, depends accurately or approximately on this parameter p to the power r. Then see Taylor Series Expansion says set the new matrix A due to parameter modification of Delta P is the original matrix A plus C summation, which we seen in the center of this view curve, where the partial derivatives of this matrix A with respect to this parameters, is used. So delta p_i is a different parameters are then applied to. The crucial point here is to determine this gradient dA via dp, and these well-structured problems can be often done by so-called semi-analytical means. So this derivation, this differentiation can be done on the matrix itself, in a semi-analytical way, and semi says it's done on a computer numerically. If this does not work in certain circumstances, one can apply finite-difference. Of course, it means more computational effort, but keep in mind that difference can be done in parallel on many processors in parallel and then again, at least computational time, wall clock time is saved. What's also important is say, possibly, the chain rule should be applied if one knows and if one can estimate the influence of this parameter on these different matrices, namely; let's assume if this parameter is a one over P influence, on inverse influence. Since the gradient of this matrix A on the coefficients of this matrix A as function of this parameter P and the gradients are obtained via this chain rule. The original coefficients divided by p squared with a minus sign and all other terms of this matrix A which are not influenced by this parameter are set to zero. It's a partial derivative, and let's assume alternatively since this parameter influences since matrix A in a quadratic term since the chain rule tells us that the original coefficients of this matrix A can be determined by multiplying is coefficients with two parameters p. Again, because it's a partial derivative, all as a parameter is not influenced by p's are set to zero. This way of expanding and parameterization very much determines the quality of the interpolation and the parameterization, and the quality of approximation itself.

Let's look into this digital Model Order Reduction and at the same time it's parameterization, more in the sense of a flow chart. First of all, on top, we are starting with an overall system modeling. It might be a finite element model of computation, fluid dynamics model, a coupling of both and so forth. Any proper matrix modeling method can be used. Then we have to consider the projections, the model reduction and its parameterization. For finding such a projections there are different methods. It's just previously mentioned. Departmentalization means either Taylor's pension or interpolation technique and so forth.
Play video starting at ::54 and follow transcript0:54
Once these reduced model, and these reduced parametrized model has been established, the simulations can be done on this reduced model. It's not only the simulations, but essential is and keep in mind for practical applications in the real world development processes is, one has to consider the iteration steps on modifying the parameters. So one has to repeat it again and again, but on the contents of reduced model, doing all these steps. We have to know, is it good enough? Can it well represents the behavior of the system? Can it well represents the behavior of the system even after modifications of parameter? So we have to assess the reduced and parametrized model. Often used technique is to look into relevant transfer functions, especially in computational and structure interaction problems. The transfer functions of loads to responses, the transfer functions of actuations to the the system responses and system outputs are looked into, with and without model reduction. Here in the center of this u-curve, we see such a typical transfer function over the frequency of the system. Originally it calculated with blue curve, which is hidden behind the red curve, if you wish so, which represents a large-scale models, where 1,400 so-called states or modes or Eigen frequencies or vibrations are determined. On the reduced model, only 26 states or Eigen frequencies and modes are used, and you see that such a transfer function and the representation of the significantly reduced model and the original larger scale model can be assessed by a transfer function. At least it tells us set up to 20-30 hertz, these reduced model works very well. If we are looking into this assessment, we have to as always, when one is looking into assessments, to establish assessment criteria. Which means in this case, we are looking into the coverage frequency band and the covered modes. Covered means into relevant frequency band and relevant modes, effective masses are the residual modes which are not considered, and formal control point of view, of course, controllability and observability plays an important role. Because we are talking about parameter variation and parameterized reduced order model, we also have to look to which extends is parameterized Reduced Order Model valid for these different parameter changes? In order to better understand what we mean with this, gust load behavior or this aero-elastic behavior and the turbulence. You see here the performance and the behavior of simulation model. Namely, when we start this model, we see that this aircraft and this finite element model of this aircraft is our elastic model of the aircraft experience a rigid body deformation, which also causes or may cause some discomfort of the passenger. But also we see at the same time, the wings of the aircraft are vibrating and get an elastic deformation in the vibration, dynamic deformation, which in this case is relatively small, but there are other cases where these vibrations might become large. Now to make things more concrete, we are looking into series of applications in aerospace systems. Not only from a model reduction and motor condensation and parameterization point of view, but also I will address some points of the physical and technical background of these applications. Let's start again with this is aircraft model and aircraft simulation model and behavior model, where we are looking into different lift load distributions. As mentioned in the introduction, we have three different lift load distributors. So first one is the so-called stationary one. When we neglect the fuse large influence, it's elliptic lift distribution or weathervane. Assuming that the aircraft is flying into turbulences and into a gust, the wing might talk and the angle of attack might increase, especially at the very edges of the wing. This means that the lift is increased at the outer part of the wing. The overall lift is increased, which causes not so comfortable behavior of the passengers, but at the same time also causes an increase of the so-called bending loads and bending forces at the wing root, which are shown here in these red diagram. So in order to reduce this, this gust load defect, what is looking into such a load control technique, which are different ways to achieve this, namely the so-called passive way via aero-elastic tailoring. The wing is designed in such a way, and the parameters of the wing are determined in such a way that due to the natural pressure distribution over the wing, the wing is deformed in such a way where this negative effect of gust loads is minimized and reduced. Alternatively or additionally, one can do this also by active means, by properly controlled use of flaps, where these flaps are used to enforce these proper bending torsion behavior to get a good overall least distribution which minimizes and reduces the bending moments at wing root and also reduces the not so comfortable behavior for the passenger. From this behavior, we see that these strong interactions of aerodynamics and vibration has to be avoided. So these flutter cases, as they're called, have to be avoided not only in order to avoid large deformations, but also to avoid such large forces, such as structures, ring and in this case also the bridge starts to break. So it's a modeling of this behavior results into so-called Aero-servo-elastic simulation models. Aero means aerodynamic, servo means control and activation, and elastic means evaporation, deformation behavior of the aircraft. This means that we first of all have to establish finite element simulation model for structural dynamics, which you see here for an aircraft are so-called half model. On the left hand upper side see differential equations in time into left center side, and also including rigid body modes on the lower part. The upper part of this aircraft shows an elastic eigenmode vibration frequency of the ring together with the smaller amplitude in the fuselage assuming this structural dynamics behavior to be validly represented by these models, we have to also take into account the aerodynamic behavior. It might be either steady behavior or especially in gust, it's an unsteady behavior. So then takes usually the doublet-lattice method or any other computation fluid dynamics analysis technique and these too large molecules have to be combined and coupled together, and in order then to carry out the parametric studies, one does not want to treat these investigations on these two coupled large-scale models, but on the reduced model and the parameterized reduced model like the determination of the eigenvalues, which are sketched on the lower right-hand side, whether they are the real parts, whether they are the imaginary parts of these eigenvalues. So what does active dynamic load alleviation mean technically for the aircraft? This can be seen in this figure and this is sketch for this plane to embody configuration where we assume set is planted to embody aircraft is excited dynamically. Why are aerodynamic disturbances? These disturbances are measured by sensors in front of the aircraft, for example, by certain radar techniques or pressure sensors, but the response is also measured by acceleration sensors in the wing, which gives for the feedback loop of this overall control loop, the sensor inputs of this behavior of the wing. By applying send a feed for combined, feed-forward, feedback control loop, means set the overall response, the dynamic response of serving can be drastically reduced. This is shown here in this transfer function. Where typical displacement at the wingtip is shown versus the frequency of this wing, the resonance or high frequency of this wing and we see here is this blue curve where no active action is taken, no flaps are used to counteract the disturbances and the influences of the gust. When the control loop is applied, which triggers a flap such that this influence is reduced. This is shown in the red curve and we see that practically the overall response now is only half of that of the uncontrolled behavior. The next application is taken from Space Engineering. We are looking into the vibration control and the vibration design of very flexible solar arrays. I'm saying very flexible because these solar arrays as one can see here on the left-hand side of a satellite in orbit, of course, it's not a view in the solar array on the ground in a test or in the test laboratory. Since solar arrays are quite large and floppy structures, so the eigen or resonance frequencies are also very low, but they might be even smaller than a tenth of a hertz. So in case such solar arrays start vibrating, they might introduce disturbances on the satellite, which one wants to avoid to the most possible extent because it might cause small tumbling movements of the overall satellite, which influences its overall performance and behavior. We were looking into more detail, and we are looking into more detail of such a solar array panel. Namely, you see here on the right-hand side, such as singles solar array sandwich panel, which we want to control in its vibration behavior both in orbit and during launch and for investigation, we first started with the overall or full-scale model and we simulated the behavior of and the influence of time parameters of such a sandwich panel named as Young's modulus, thickness distribution of the sandwich panel, and also possibly positioning of non-structured buses. The original simulation model at more than 11,000 degrees of freedom and the drastically reduced model, which we were looking into and handling together with parameter studies and together with interfacing this controlled behavior, only has finally had 51 degrees of freedom. What we did, we're looking into different modes, vibrational modes also, among others to determine proper positioning of actuators to control vibration and control always means you always have a proper positioning of sensors in order to guarantee good observability of the behavior of this plate.

As mentioned in our introductory part in our basic discussion of the methods, the validity of this reduced and parameterized model has to be checked. Of course, this what's done here also. Namely different parameter modifications of system parameters have been carried out. You see these parameter modifications were between zero and 40 percent depending on the different type of parameters. What's given in the main graph are so different frequencies and modes, starting from frequency in mode number one up to frequency in mode number 20. The lower curve shows the behavior, the frequencies without parameter modifications. The two upper curve is which practically match each other, shows the behavior of the results of full model simulation and parameter modification, and reduce model simulation parameter modification. You sees well matches with each other. So this gave us sufficient confidence that at least in this variation range which showed at the upper part and left upper part of this graph, this could use model and parameterize that use model can be reasonably applied. As always in development of aerospace systems, we should correlate once hardware's available. You should correlate the simulation results with the test results, and this is shown here. This is the transfer function of such a sandwich panels versus frequency, where the response is shown with uncontrolled vibration behavior, these as a large peaks. Blue and red curve shows the behavior under vibration control, and then become correlate this with tests on such a panel. You see that this behavior from simulation from test correlates very well. These are nearly quite identical curve, but, of course, it can apply different mathematical criteria for correlation, but also this qualitative correlation tells it's quite good. By the way, this is control and vibration control in that case, was stand by applying control actuation via attention piezo-ceramic plates on the sandwich and properly clicker, which is a control loop. Our final example is taken from high precision Cloud telescopes and their active optics. What do we mean by this? There's just a large Cloud telescopes under development in Europe, which is called the Extra Large Telescope, ELT. This Extra Large Telescope has a main mirror diameter of close to 40 meters, which is quite large and a big step in astronomical telescopes.
Play video starting at :3:19 and follow transcript3:19
This large mirrors have to keep their shape very accurately even under very different disturbances. In order to guarantee this, one is sectioning this large mirror of 40 meter diameters into many small sections. We see here a single panel or a single section of these large mirror, a few from the rear side. First of all, if you have a closer look, you'll see it's a finite element modeling in this case. We also see some structure on the rear side. We also see actuators are integrated in order to control and counteract the deformation of such a panel due to external excitations. So it means that one has to deal with very large models, and one has to deal with control structure interaction problems, which in the end require many thousand, even more than 10,000 actuators. This is very large models have to be properly condensed in this overall simulation on structural contour and optics behavior. The overall integrated model structure contour optics end-to-end model of this telescope is shown as u-curve. Take first instructional model, the optical model, but also we need to properly model the actuators, which are attached to the rear side of this main mirror. We have the controllers and so forth. We have to take the disturbances, which are wind load seismic disturbances, even solar pressure and thermal behavior, temperature changes. But it's also interesting to consider this disturbance shown on top, which is the so-called atmospheric turbulence. The atmosphere always is turbulent, which means the air has different densities and optical rays going sources areas different densities are disturbed. Since it's also to be compensated via active control technique. As mentioned, we have now to assess the validity of this reduced model and the validity also with respect to parameter variations. See validity can be valid checked via transfer functions. We have seen this graph already. It comes from this large content scope. Remember, the original model had, with respect to finite element degrees of freedom, 500,000 degrees of freedom. So its used model only in quotation mark now has 200 degrees of freedom. Or from a control point of view, in the original model might be determined, 1,400 states. While in the reduced model, we only determined and had to be determined only 26 states. This transfer function shows us that it really works for frequencies and eigenfrequencies, which are relevant here from structural optics point of view for up to 10, 50, 20 hertz.

What's important now is the result or the behavior of this large mirror, under different external disturbances or loads. So left hand side, shows the formations of this mirror view from top of this mirror, with isocontours of the formations, they are in the nanometer range. But we see that they are differently distributed over the overall surface of this mirror. While once contour is applied to each of these segments, this behavior is very smooth and says practically no real variations and practically know the formations of this mirror. This very well also shows in the optical results, in the astronomical results, the lower figure and lower picture shows us photograph of an observation made from such telescopes from count into a stellar object. So practically, is the same photograph taken from a satellite or space telescope is shown here. You see it's even a little worse, mainly because one can put in a lot of control activity, actuators, and so forth on count telescopes which in the visible wavelengths and in the near visible wavelengths, very valid improves the behavior of this con telescopes. Let me summarize with some final remarks and a brief outlook. First of all, I think we have seen that reduced order models and especially so also parameterized reduced order models can be beneficially applied to different aerospace systems and especially when several disciplines are involved in so-called multidisciplinary problems, structures, fluid dynamics, contour things, and so forth. We have also seen that also there is a reasonable need for parameterized reduced order models to carry out these parameter studies that is optimization also on the level of reduced order models. Of course, the final step should be then done again on the full model. We've also seen I think that the whole process is not really a automatic process. But one has to introduce significant engineering knowledge in order to investigate and establish a proper sequences of sub processes in this context. It's good since this knowledge has to be introduced because it means that these engineers and people are doing this are still needed with their knowledge. One example for future research activities is that people are more and more looking into doing reduced order modeling and parametrizing in non-linear systems, and also doing contour on this level. So things which I have shown you are not only done by myself only, but they are contributions from different doctoral students which I had in our institutes at TU Munich, and I would only like to mention two of them. It's Dr. Eun Jung Yoo and also Michael Muller who did these activities in the large count telescopes, and I would not only like to thank our doctoral students, but I would also like to thank the audience for taking part in this video session. Thank you very much. This final list of literature which is very short lists by intention, shows you said there are further sources and references and textbooks available you might look into. I would specifically like to take your attention to this toolbox, which is published by the Chair of Automatic Control of Technical University of Munich. Their homepage you have is also given here, and you can find here as well. Thank you very much again.

# Parameterised surrogate simulation models in Aerospace - Soft computing methods for weakly structured problems and models
Welcome to our lecture on parameterized reduced order digital models in aerospace, which is our second lecture on this matter, which we call the Part B. Where we mainly focus on so-called soft computing methods to establish such models. Again, this lecture is part of our Munich Aerospace contribution on the online course digitization in aeronautics and space. So aerospace systems are characterized by a wider set of different parameters, namely, operational parameters, which have to do with the payload mass, the flight mach numbers, the flight height, flight level, center of gravity of the mass, and also set design parameters which mainly relate to topology, shape, geometry, material properties, and so forth. In the end, design and development means to select these parameters, such set requirements are satisfied, and design goals are achieved. This process, which might take several years to come to an end and to an optimal solution, is heavily based on digital models and so it's a digitally driven development process. Why do we have this lecture parts A and B? You might remember in our lecture part A, we were focusing on so-called large-scale discetization methods which result into large matrix equations and these matrices are significantly reduced in size, which means that it's the lower order model or reduced order model. Today we will discuss so-called surrogate models, which have a little bit different approach. Mainly, parameters are also varied. Maybe the exact response for all these parameter, system response for these parameters is obtained by high fidelity models. Then these certain set of such reference points and the reference values approximation functions, are established which help represent the behavior also coverings the variation of many different parameters around these reference points. So our goal is a drastic reduction of the computational effort biases, surrogate models. But at the same time, there should be sufficiently accurate and the deviation between assumed accurate results and those coming from the surrogate model shall be small. We assumed high fatality response, which we assume to be accurate, shall be Y and V. Approximate response shall be only deviate from these with small approximation error. But again, since in the development process we are varying these parameters, our design and operation parameters, this approximation shall be also valid under this variation of parameters or it might even be that we have to update our surrogate models from time to time. Let me discuss the basic concept of the soft computing approach and establishing surrogate models. Again, we start with parameters one and parameters two, unfortunately, when explaining something. This set of parameters, by explaining it graphically, we cannot usually use more than two or three parameters because one cannot easily represent four parameters graphically. So let's assume we have two parameters describing our system behavior. So relevant response to relevant output, and from certain sources, either from a high fidelity models or other sources we have and we can get the behavior of the system and the response of the system assuming quite accurately. To use this reference values to interpolate and to establish interpolation function and polynomial function, which not only covers these reference points, but also covers variations of such parameters that we can use these interpolation functions as a kind of surrogate model of our parameterized system. What also gets obvious from this graph is that this process and its quality is affected by different aspects, namely, the distribution of these reference points plays the role and the number of these reference points plays the role. It's obvious it's the largest number this of these reference points better the approximating polynomial would be. But on the other side, we want to limit effort to determine reference values, possibly as, and especially also with high fidelity models. The type of these approximation function of these which is shown here in a mesh shape, also plays a big role. So better these approximation function represents a real physical behavior, technically behavior, the better the approximation function will be and the better its validity we have also for varying parameters. But still open is were do these reference values come from? There are different options. As mentioned, they may come from high fidelity models, they may come from experimental data, but they may also come from educated guesses and also knowledge and insight in development teams. When there is different specialists around and one could via discussions. I will show you an example. But could also established such surrogate models. So on the one side it's then once these surrogate model is established, it is a fast, evaluatable and fast can be quickly evaluated and it's better, it represents a real behavior, the better it is also for larger variations of this parameters. But this process can be quite cumbersome if many different responses are relevant, that's one thing. But on the other thing is, if we determine these reference values via high fidelity models, which we want to limit in quantity, we can still use so-called parallel processing. But because there is no requirement to determine these reference values in sequence, but they could be determined in parallel on so-called parallel processes.


Let me first explain these use of multiple processor systems or these parallel determination of these reference values of high-performance computer systems with many processors. Here we see such computer systems. On the left-hand side, we see such collection and such a cluster of many processors usually applied in departments and also in some special cases, very high-performance clusters might be available where even thousands of processors are available. But such is not standard and I will not go in more detail on this. But irrespective of this, the basic idea is the same in both type of high-performance computers and multiple processors. Namely, this crude or grain parallelisation can be used by implementing the high-fidelity model on each of these processors being either 50 or 500 or 5,000. These models then are one in parallel with different parameter inputs. Or in other words, is different positions of our reference points in the parameter space. So in the end, we get distributed responses or distributed reference status on many of our reference points in the parameter space simultaneously. So once we have then the surrogate model established via some interpolation through the points we have these reduced or surrogate model and we can have a digital twin, as it's sometimes also called, on the run where we are no longer relying and have to no longer rely on high-performance computers. So let's jump into more details on constructing such response surface or establishing such surrogate models. First of all, we have to establish a polynomial function as a function of our design and operation parameters. Since polynomial function relationship has opened parameters which we adjust, use the reference values, namely these a's and this exponential data beta. Often it's assumed that these a's are zero if they are the a sub for i not equal chase or there is no direct coupling of the parameters. But it's simplification but it's not a strict condition. But what's important is we have determined in advance reference values from, let's assume, high-fidelity models and since reference values are we capital our values. This polynomial with its open parameters a and beta are now determined by a least square problem, where we want to have the polynomial to represent these reference values assuming these to be the exact values coming from the high-fidelity models with matchings value it receives. It's a least square problem. So number of reference values we need is about 1.5 times the number of these open parameters of the polynomial function. Be aware of once we have determined this polynomial functions fully, namely the a's and betas, then this ansatz or this function is an explicit function as mentioned in the very beginning of our lecture today. So its can be quickly numerically evaluated. It can be also for further evaluations like for differentiation and so on, can be handled very quickly and very straightforward. In this case, we assume since beta's in advance, then only the determination of the a's is left for this polynomial approximation. This leads to the special but still relevant case of a linear least square problem for these a's.
Play video starting at :4:32 and follow transcript4:32
What is recommendable is that putting into these ansatz with this star on the top of this view curve, engineering and physical knowledge of this behavior. So these exponents beta shall be selected such that they represent the expected behavior of the performance of the response of the system with the design and operation parameter as p. So let's assume we have selected an initial set of reference points but it might come out during this parameter's design, parameterization and even optimization process. Said it comes out, sets this infill points have been not well chosen with respect to positioning and quality of these response services and of these polynomials we use for interpolation. This leads to the point of infill reference points. Namely, let's assume that we have determined the initial set of reference points, for example, by the design of experiments technique and the numerical simulation is also still assuming the high-fidelity model, we determines the behaviors, the performance, the response of the system which we then use as reference points to construct these interpolation equations and polynomials and use this for further design and parameter studies. But let's assume we find out by different means that the validity of the surrogate model and also used reference point is not sufficient. So what we could fill in, first are reference points. So this process might be repeated in order to update and improve the surrogate model. There are different criteria for selecting such infill points; maybe different parameter changes we have carried out during the development process, different parameter variations and modifications, maybe we have used even optimization methods and routines to vary these parameters. Those are development and design process. We could use a selection of these as infill points. But we could also establish and use some statistical and probabilistic means to estimate where to position such infill points, where we could expect the largest improvements in the approximation model or in the surrogate model, which then is improved and can be taken further for the parameter and optimization, that is, in the whole development process. But we have to keep in mind once we are talking about technical systems, we have allowable parameter variations and combinations and not allowable or infeasible ones. So it would make sense, it holds our initial and infill points would be feasible points in the design and operation parameter space. Use of infill points, let's make this more clear by assuming a simple example with the one decay with a single parameter which is varied and where we're applying surrogate models and polynomials to approach and approximate the real behavior of the system which we don't know explicitly, only implicitly. We assume the real behavior as function of the single parameter, giving on the x-axis on the abscissa, to be the red curve. Our initial reference points would be these three points in the center of this graph, if you see stars. The polynomial function, the surrogate model would be the blue hashed line, and it's not really a good representation or with the whole spectrum of the parameters. So further infill points are introduced, two further infill points on the right-hand side and one on the left-hand side, and you see the blue curve better approximates the red curve, which is assumed to be the real behavior, and if even further infill points are applied, then the approximating curve, this hashed blue curve, practically batches with that curve which is assumed to be the hypothetical one, but we don't know the explicit behavior of this accurate behavior because everything is mottled numerically via digital model. So far we have discussed constructing this response surfaces or the surrogate models by using reference values to determine initially from high fatality models of course, in a limited number of reference points. But this is not the only means to determine reference values and the quantities of this reference values and the reference points by high fatality models, but one could also apply maybe in parallel or in addition, the use of qualitative and a specialist and knowledge based engineering models and knowledge by transferring this qualitative knowledge into numerical models which then are established also by interpolation functions and by response surfaces. To make this approach which is a little bit different from a computational point of view, the best thing is we use an example. Namely let's assume we want to develop a propeller blade of a propeller aircraft. That set is propeller blade has best performance, can be relevant, in fact that doesn't cause problems in strings and in elasticity and in vibration and the force. So the design problem would be, we want to maximize and select design parameters of this propeller blade, geometry data, material data such that it, for example, maximizes its first eigenfrequency, of course, considering the rotation speed of the propeller blade, and we want at the same time to minimize the manufacturing effort, assuming here that manufacturing is made of this propeller blade by operating process for carbon fiber reinforced plastic or glass fiber reinforced plastic. So the parameters from an engineering point of view to be determined are the braiding angles and the angles of the reinforcements resulting into also having consequences into the braiding process, and the individual thicknesses of this fiber composite materials, and fiber composite layers being either glass fibers or carbon files. But it's always in addition to design goals, on eigenfrequencies, and manufacturing effort here, you always have to consider, generally, in development of aerospace system, you have to consider constraints. In this special case, we have constraints of the forces acting at the propeller hub, on the deformation of supply passe under the different loads on its overall aeroelastic behavior, and of course, on its strengths and so forth. Let me, as an intermediate step, briefly explain this braiding manufacturing techniques and the relationship between the geometrical design parameters and the manufacturing efforts by braiding. Here on the left-hand side and on the center, we see such a braiding machine. It's a large wheel where the different fiber rovings are brought in, and these fiber rovings are placed in proper angles on this mandrel. This angles which are shown also on the very right part of this figure are not only design parameters determining the material behavior, material properties and this is the structure of the aeroelastic behavior of the propeller blade, but these are also determining the manufacturing effort. So as a intermediate step, let me also assume that, which is often the case, that at the very early development stages, we do not have sufficiently accurate, if at all, simulation models to determine the manufacturing effort of such components and parts of an aerospace system, and this might be also valid for our other blade. But what's often the lund is that we have expert lund which can give educated guesses for the behavior of the manufacturing effort under variation of different design parameters. So we have manufacturing efforts we are discussing with, we have the knowledge engineer who is the guy who helps to transfer this qualitative knowledge of manufacturing effort into numerical models, and we then have the design optimization effort expert, which is combining sensis numerical models of manufacturing effort, it's combining it with the structure and aeroelastic simulation model for the overall and multidisciplinary optimization process.

So this is interviewing and collecting verbal knowledge and then finally transferring it to computational and numerical models is mainly based on fuzzy logics. Namely, one asks us specialists, experts. Let's assume to be a certain parameter, one or two parameters out of our design and operational parameters set to be either small, medium, or large. Then please give your assessment on which consequences does it have on manufacturing. Maybe we are even asking two or three or four different experts and these results and these replies and are then used which we will see in a certain way to establish. Finally, response surfaces or surrogate models. Resulting forms is qualitative knowledge and resulting forms is fortune fuzzy logic replies and see in principals is soft computing techniques could be applied and used also for other complex system behavior. So the condition is that there are some experts around who can make educated guesses. So these experiments are presented different parameter sets, being as a small, medium, or large. Assuming this parameters do occur in a small to a very limited probabilities or the decree of membership of small parameters might be small. By large parameters, large reinforcement angles might occur quite often. So as a membership of a large parameter, a membership probability might send be also large. From their reply and their assessment and educated guess on what does it mean for manufacturing of what such response surfaces and approximation models can be also established. Source fuzzy knowledge means, we have input parameters 1, 2, 3, 4, 5 geometries and thicknesses and whatsoever and we are prescribing assumes them to be small and assume them to be larger and assume them to be medium and assume them in this and set combination. What's your guess? You as a specialist as in output in our case, the manufacturing of what is it small, which is given here as short or is the manufacturing effort large? From this interviewing and from this database may be established from different experts, one can also construct biases, fuzzy logic techniques, such response surfaces, where, for example, geometrical parameters are combined together and are represented by the relevant respond service, which keep a sense of qualitative knowledge numerically about the manufacturing effort, which for simplicity is here used as a criterion for manufacturing effort to manufacturing time of such an auto plate. So combining these different type of models, high-fidelity, low-fidelity qualitative model is determined by such soft computing and interviewing experts. We are finally achieving optimization results by applying optimization algorithms and parameter modification techniques on these different models which represent our behavior in that case the physical performance in mechanical performance of the auto plate and of this propeller plate, and interrelated manufacturing effort. Both switch one to optimize as a design goal. Since we have two different design goals, as a result of optimization can only be a compromise between these two different design goals, which we see from this what's called Pareto frontier, this red curve which is given over. So you first design goal manufacturing effort but the other design goal, fundamental frequency, we see if we only spent and are ready to spend smaller effort on manufacturing, we have also achieved as the science, the best possible design with respect to fundamental frequency, which is about 45-48 Hertz. If you need a higher frequency of our auto plate, we would have to spend higher manufacturing effort. Let's say a manufacturing effort not 0.3, but 0.6 quite as large. But at the same time, we achieve fundamental frequency of 65-70 Hertz. All this optimally compromises. This compromises between two different design goals. They lie under this red curve, which is called the Pareto frontier or the Pareto curve, and which helps to assess the overall design and to select proper compromises which well represent the manufacturing effort and the limit of manufacturing effort on the one side and achieving good overall physical and mechanical behavior of the auto plate on the other side. Since the manufacturing effort has been modeled by such a surrogate model obtained by knowledge of qualitative knowledge of experts. Why all these is fuzzy, old techniques? While you can also identify where does high manufacturing effort come from, is it moister layer one, layer two, or layer three? Results can be also seen, which very much gives a much better insight into the overall behavior and then to the overall behavior of these two design goals on performance and manufacturing effort.

Now, let's come to our final example, but this final example also allows me to introduce a further aspect to use certain techniques to establish such reference points and reference values to properly establish surrogate models. This example is taken from aircraft configuration tasks. How does an aircraft have to look like in its overall behavior and in its overall properties, geometrically and other type of properties, such that this aircraft is good, if not the best aircraft, to be developed under the required goals and constraints and conditions? So configuring global aircraft parameters usually means taking into account different criteria, which means in the end, different disciplines, it's multidisciplinary problem. We have to consider the aerodynamic behavior, we have to consider weight, and balance, and mass and so forth. The overall performance of the aircraft, the efficiency of the aircraft if you wish so, it's flight stability. This aircraft and this configuration is described and has to be described often by quite a number of parameters, global geometry parameters and others, which we will see in a minute. Since parameters might be determined by parameter variations in order to achieve good design goals and to satisfy requirements. But once we have established related models, forces, in that case for different criteria, these parameters could be in principal also determined by optimization algorithms like all the genetic algorithm. Once we have available models of these four criteria as function of said design parameters, and since models can be unusually are also surrogate models. So such a configuration task and its parameters are given here as an example as a parameter list. So design parameters might be wing span, the wing areas or very global overall design parameters, wing sweep, for example, the horizontal tail, tail area, but also fuel mass, cruising altitude, and also the arrangement of seats and of exits and all this, which will later heavily determine the overall performance. For all this activity, it's recommendable not to start with large set of parameters, but to increase a set of parameters continuously during the development process, because starting with a large set can only means what has to put in a lot of effort to establish the relationship between the performance criteria and these parameters. But also at the very beginning, it might not be easy to assess validity of the computational results and numerical results on gets from this modeling, and parameter modification, and parameter optimization effort. As I mentioned in addition to this global geometrical parameters, we also have internal parameters within the cabin, within the aircraft, like number of seats, arrangement of seats, and so forth and so forth, which of course also determines the overall performance of the aircraft from an aircraft operational point of view. So we have in this four criteria on aerodynamics performance on weights and masses, and on flight dynamics, simulation models, which might be in certain circumstances high fatality models or might be surrogate models. Or from the beginning there, there might be, let's say, for example, for weights, for such configurations which are standard configurations where in the last 50 years, a lot of experience has been accumulated and collected in statistical databases which can be used as a surrogate modeling. So from the beginning, especially in such initial configuration tasks, the simulation models often are from the beginning surrogate models, for example, also aerodynamics is not necessarily from the beginning modeled via complicated computational fluid dynamics, but like a technique, like a Vortex lattice method which is approximating the aerodynamic behavior in a sufficiently accuracy, but doesn't take too much effort. So we have from the beginning surrogate models, and surrogate models combinations but with different results for all these different disciplines, and we have, because of this parameter variation, because of this optimization steps, on the one side, these relatively small models. But since we are investigating many parameter modifications combinations, we have many design points to evaluate. So the result of these are large number of data. Nowadays, it's called big data assessment, big data evaluation. So what this allows me to bring in the idea of using techniques of big data evaluation to establish surrogate models for such types of design and configuration tasks. Since two different methods, genetic algorithms for optimization and data mining now to be applied on this large database. To be applied not only to establish surrogate models, but to be applied to speed up the convergence of this genetic algorithm and optimization techniques is done in the following, namely the one side of genetic algorithm is designed driver, which modifies the design parameters. It also can handle, such genetic algorithms, can handle discrete design variables like the number of seats in the cabin, and which is also often the case and also with this simplified models in these four disciplines, it can also very well use weakly structured problems. We don't have gradients of this behavior, of these responses with respect to the parameters and the force. But on the other side, genetic algorithms usually need a large number of evaluation design point evaluation. This large number of evaluation shall be reduced by using data mining technique to evaluate large data sets. Data mining helps to identify relationships and associations between the different parameter sets especially those which leads to better design parameters, which helps to select favorable modifications, favorable mutations of design parameters in this genetic algorithm. We see here the consequence when using such data mining techniques, namely the blue curve is data mining switched off. We see here the genetic algorithm needs 40, 50, 60, 80 generations if not more, to achieve the optimal results and each generation might have 10, 20 or so different number of population. So it means a lot of evaluation of data positions of the parameters in our design space, not so much the effort pair evaluation, but a large number of evaluation. But once the data mining is also used to improve this mutation, this valid modification of the genetic algorithm which modifies the design parameters, once we use this data mining techniques you see here on the curve with the black dots, so that we achieve practically after 15 at most 20 generations, we achieve the optimum result. A typical configuration task with this many design parameters on the one side and criteria's on the other side, might be the following configuration task. You might have to determine this design parameters such set on the one side fuel consumption is small and on the other side the block time is small. I'm talking from the docking device at the starting airport and talking to the docking device at the airport we are finally flying to. As always these parameters not only have to lead to good design goals and good compromises between these, but they also have to satisfy certain constraints such that they do not lead to unreasonable design parameter ranges or even satisfy constraints which makes its flight behavior unstable. The typical result from such activities shown here. Again, since we have two design goals, it's a Pareto-optimal curve, where on the x-axis, the optimal and minimum possible fuel consumption is shown and on the y-axis the optimal and minimum block time is shown. So if we are requiring that we have minimum and small fuel consumption, we see that we have to select something between 20 and 30, whatever it is, tons for example on fuel but this also means that this results into configuration which might lead a higher block time because then the overall configuration is such that the passengers take more time to go into the aircraft and to leave the aircraft. So it also influences block time. But if we are reducing and want to reduce the block time, we have to spend more on fuel consumption and in that case we are reducing block time which we see on the more right-hand side of this Pareto curve. We see that the length of the wingspan is smaller. The smaller wingspan allows to operate more quickly more fast on the airport which reduces the block time. But at the same time aerodynamic performance is not as good. So fuel consumption might be higher. This shows that we have to select the proper compromise and the financing configuration we are happy with and which we find have a combination of these design goals to be a very good and optimal compromise. So let's conclude. First of all we have to be aware and might not have been always obvious during our discussion, let's say some preparation work to establish surrogate models like determining the reference values, the reference points like determining and transferring the qualitative knowledge to quantitative models via knowledge-based engineering and so forth.
Play video starting at :12:40 and follow transcript12:40
Once these surrogate or the produced models as functions of the design parameters into design parameter space are available, the evaluation can be done very fast independent if you wish so on large computers, and we can also handle by this multi-disciplinary tasks. We have also seen that for establishing such models,
Play video starting at :13:9 and follow transcript13:09
different methods can be applied like SOS. First, establishing reference data with high fidelity models like SOS, establishing reference data by knowledge base engineering, and by transferring qualitative knowledge into quantitative models into force. We have also seen that different elements of such techniques can be favorably combined. Namely, the example of combining the genetic algorithm with data mining techniques. The convergence speed of genetic algorithms can be significantly improved and enhanced by using data mining techniques to improve the mutation step in such genetic algorithms. But I want to make you also aware that once these models are established, the quality of these models have to be validated somehow by plausibility checks, by cross-checking with high fidelity models, by checking with experimental results once flight tests are done, and also by feedback from the operation behavior of this aerospace system. But taking all these together, we can expect that these soft computing techniques and these use of big data techniques like data mining, this use and application will be even coined in the future in order to cover a broad scope of multidisciplinary, multi-criteria design and development problems in aerospace based on simulation. Okay, this view graph shows you a rough selection of literature. There's a vast amount of literature available on this subject but this is a selection out of these. Some of these data and literature and papers are written by our doctoral students may be with participation of myself and that's one thing. What's also very important to note and to mention is that for these aspects we have discussed also software at least in its basic format is available. For example, in MATLAB where everyone can see and find something on surrogate modeling, where one can find something on data mining, where one can find something on genetic and other type of optimization algorithm into force. So you don't have to start from scratch for using such methods and technique. Last but not least, I want to thank you for participation in this lecture and participating in this presentation. Thank you very much.
